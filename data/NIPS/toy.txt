crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data Crowdsourcing is cheap and fast but suffers from the problem of low-quality data To address this fundamental challenge in crowdsourcing we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest we show that surprisingly under a mild and natural no-free-lunch requirement this mechanism is the one and only incentive-compatible payment mechanism possible we also show that among all possible incentive-compatible  mechanisms that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers  Interestingly this unique mechanism takes a multiplicative form the simplicity of the mechanism is an added benefit  in preliminary experiments involving over several hundred workers we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure
convex potential minimisation is the de facto approach to binary classification However Long and Servedio [2008] proved that under symmetric label noise SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing This ostensibly shows that convex losses are not SLN-robust in this paper we propose a convex classification-calibrated loss and prove that it is SLN-robust the loss avoids the Long and Servedio [2008] result by virtue of being negatively unbounded the loss is a modification of the hinge loss where one does not clamp at zero; hence we call it the unhinged loss we show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM and is the limiting solution for any convex potential; this implies that strong regularisation makes most standard learners SLN-robust experiments confirm the unhinged loss SLN-robustness
one of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience This includes the necessary and sufficient conditions for generalization from a given finite training set to new observations in this paper we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions we provide various interpretations of this result  For instance  a relationship is proved between stability and data processing which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning in addition we establish a relationship between algorithmic stability and the size of the observation space which provides a formal justification for dimensionality reduction methods Finally we connect algorithmic stability to the size of the hypothesis space which recovers the classical PAC result that the size complexity of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization
we develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori we present an easily computable closed form parametric expression for the conditional likelihood in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors Motivated by large-sample asymptotics we propose a noveladaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate we further prove that in the large-sample limit the conditional likelihood and datapredictive distribution become asymptotically Gaussian we demonstrate through experiments on synthetic and real data sets that our approach is superior to otheronline state-of-the-art methods
Monte Carlo sampling for Bayesian posterior inference is a common approach used in machine learning the Markov Chain Monte Carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations SDEs). These SDEs are guaranteed to leave invariant the required posterior distribution An area of current research addresses the computational benefits of stochastic gradient methods in this setting Existing techniques rely on estimating the variance or covariance of the subsampling error and typically assume constant variance in this article we propose a covariance-controlled adaptive Langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution the proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications